<div style="text-align:center">
<img src="AFP.png" alt="\sth\AFP.png" width="600"/>
<h2>Improving Neural Network Efficiency via Post-training Quantization with Adaptive Floating-Point</h2>
</div>
pytorch implementation of Adaptive Floating-Point for model quantization

# Overview

## Citation

We now have a [paper](#), titled "Bit-Flip Attack: Crushing Neural Network with Progressive Bit Search", which is published in ICCV-2021.
```bibtex
@inproceedings{liu2021afp,
 title={Improving Neural Network Efficiency via Post-training Quantization with Adaptive Floating-Point},
 author={Liu, Fangxin and Zhao, Wenbo and He, Zhezhi and Wang, Yanzhi and Wang, Zongwu Wang and Dai, Changzhi and Liang, Xiaoyao and Jiang, Li},
 booktitle={Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
 year={2021}
}
```

## To-do

- [ ] ***Coming soon:*** Python code.
